# LLM provider
DOKODEMODOOR_LLM_PROVIDER=vllm

# vLLM endpoint URL
VLLM_BASE_URL=http://localhost:8000/v1

# vLLM model name (must match server)
VLLM_MODEL=openai/gpt-oss-120b

# vLLM API key (use EMPTY if not required)
VLLM_API_KEY=EMPTY

# LLM temperature (0.0~2.0)
VLLM_TEMPERATURE=0.1

# Max turns per agent
VLLM_MAX_TURNS=150

# Max prompt size in chars (prevents vLLM negative max_tokens)
VLLM_MAX_PROMPT_CHARS=200000

# Sub-agent max turns
DOKODEMODOOR_SUB_AGENT_MAX_TURNS=30

# Sub-agent output truncation (chars)
DOKODEMODOOR_SUB_AGENT_TRUNCATE_LIMIT=30000

# Context compression threshold (chars)
DOKODEMODOOR_CONTEXT_COMPRESSION_THRESHOLD=120000

# Context compression window (recent messages to keep)
DOKODEMODOOR_CONTEXT_COMPRESSION_WINDOW=18

# Debug logging
DOKODEMODOOR_DEBUG=false

# Log prompt sizes (pre/post trim)
DOKODEMODOOR_LOG_PROMPT_SIZES=true

# Disable spinner/loader
DOKODEMODOOR_DISABLE_LOADER=true

# Skip external tool availability checks
DOKODEMODOOR_SKIP_TOOL_CHECK=true

# Skip running external tools (use mock data)
DOKODEMODOOR_SKIP_EXTERNAL_TOOLS=true

# Pipeline controls
# Skip exploitation phase
DOKODEMODOOR_SKIP_EXPLOITATION=false

# Claude (if using claude provider)
# CLAUDE_CODE_OAUTH_TOKEN=
# ANTHROPIC_API_KEY=
# CLAUDE_CODE_MAX_OUTPUT_TOKENS=64000
