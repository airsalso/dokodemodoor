# ═══════════════════════════════════════════════════════════════════
# DokodemoDoor — LOCAL DEVELOPMENT Environment
# Model: openai/gpt-oss-20b (MoE 20.9B total / 3.61B active, mxfp4)
# vLLM: max_seq_len=65,536 tokens, single GPU, tensor_parallel=1
# ═══════════════════════════════════════════════════════════════════

# ── LLM Provider ──────────────────────────────────────────────────
DOKODEMODOOR_LLM_PROVIDER=vllm
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL=openai/gpt-oss-20b
VLLM_API_KEY=EMPTY

# ── LLM Parameters ───────────────────────────────────────────────
# Temperature: 0.2 for dev — slightly higher than prod to help smaller
# model explore alternative tool-call paths when stuck.
VLLM_TEMPERATURE=0.2

# ── Turn Budgets ──────────────────────────────────────────────────
# 20B model: less capable per turn, but local dev prioritizes fast iteration.
# Capped lower than prod to avoid runaway sessions on a single GPU.
#
#   Model capacity factor: ~0.4x of 235B (rough instruction-following ratio)
#   Strategy: fewer turns per agent, accept shallower analysis for speed
VLLM_MAX_TURNS=150

# Agent-specific overrides
#   pre-recon: code-heavy, needs more turns with smaller model
#   recon: browser-based, moderate
#   osv/fuzzer/report: structured tasks, converge faster
DOKODEMODOOR_PRERECON_MAX_TURNS=250
DOKODEMODOOR_RECON_MAX_TURNS=200
DOKODEMODOOR_OSV_MAX_TURNS=100
DOKODEMODOOR_API_FUZZER_MAX_TURNS=100
DOKODEMODOOR_REPORT_MAX_TURNS=100

# ── Prompt Size Limits ────────────────────────────────────────────
# Calculation:
#   max_seq_len   = 65,536 tokens
#   reserve       = ~30% for completion (tool calls + reasoning) = ~19,660 tokens
#   prompt budget = ~45,876 tokens
#   chars/token   ≈ 3.0–3.5 (mixed English + code + Korean)
#   safe limit    = 45,876 × 3.0 ≈ 137,000 chars → round to 140,000
VLLM_MAX_PROMPT_CHARS=140000

# Model context size in tokens. Must match vLLM model's max_model_len (curl http://localhost:8000/v1/models → "max_model_len"). Default 32768; use 65536 for 65k models.
# VLLM_MAX_CONTEXT_TOKENS=65536

# ── Token Pricing ─────────────────────────────────────────────────
# Self-hosted local GPU — no cost
VLLM_PROMPT_TOKEN_PRICE=0.0
VLLM_COMPLETION_TOKEN_PRICE=0.0

# ── Sub-Agent Configuration ───────────────────────────────────────
# Smaller model produces longer / less focused sub-agent output.
# Truncate aggressively to save parent context space.
DOKODEMODOOR_SUB_AGENT_MAX_TURNS=15
DOKODEMODOOR_SUB_AGENT_TRUNCATE_LIMIT=15000

# ── Context Compression ──────────────────────────────────────────
# Triggers at ~71% of VLLM_MAX_PROMPT_CHARS (100K / 140K).
# Keeps 10 recent messages — smaller window saves memory on single GPU.
DOKODEMODOOR_CONTEXT_COMPRESSION_THRESHOLD=100000
DOKODEMODOOR_CONTEXT_COMPRESSION_WINDOW=10

# ── Debug & Logging ───────────────────────────────────────────────
# Local dev: verbose logging for debugging
DOKODEMODOOR_DEBUG=false
DOKODEMODOOR_PRINT_LOG_PROMPT_SIZES=true
DOKODEMODOOR_DISABLE_LOADER=true
DOKODEMODOOR_AGENT_DEBUG_LOG=true

# ── Tool Availability ────────────────────────────────────────────
# Local dev: skip heavy external tools to speed up iteration
DOKODEMODOOR_SKIP_TOOL_CHECK=true
DOKODEMODOOR_SKIP_NMAP=true
DOKODEMODOOR_SKIP_SUBFINDER=true
DOKODEMODOOR_SKIP_WHATWEB=true
DOKODEMODOOR_SKIP_SCHEMATHESIS=true
DOKODEMODOOR_SKIP_SEMGREP=false
DOKODEMODOOR_SKIP_OSV=false

# ── Pipeline Control ─────────────────────────────────────────────
# Skip exploitation in dev for faster analysis-only cycles
DOKODEMODOOR_SKIP_EXPLOITATION=true

# ── Playwright ────────────────────────────────────────────────────
# Non-headless for visual debugging during development
DOKODEMODOOR_PLAYWRIGHT_HEADLESS=false

# ── Concurrency ───────────────────────────────────────────────────
# Single GPU + single vLLM instance → sequential execution only.
# Parallel agents would queue at inference and risk GPU OOM.
DOKODEMODOOR_PARALLEL_LIMIT=1

# ── External Testing ─────────────────────────────────────────────
EXTERNAL_TEST_DOMAIN=http://localhost:9999
